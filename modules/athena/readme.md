# ğŸ§  Athena / Prometheus Development Logbook

### ğŸ•°ï¸ Initial Architecture: Transformer + PFFTTU

-   [x] Designed an initial transformer-based model using:
    -   **PFFTTU encoding**: Piece, from-square, from-square-type, to-square, to-square-type, upgrade/promotion.
-   [x] Trained on **PGN data from grandmaster games**, including:
    -   Kasparov, Carlsen, Morphy, Fischer, Tal, etc.
-   [x] Encountered key limitations:
    -   Hard to enforce legality without additional logic.
    -   Data-to-reality mismatch (real games vs supervised data).
    -   Transformer struggled with generalizing structured move legality.
-   âŒ Decided to pivot toward AlphaZero-style architecture for more grounded legal move generation and learning.

---

### âœ… Architecture & Core Network

-   [x] Designed and implemented **AegisNet** (4096-policy AlphaZero-style dual-head network).
-   [x] Identified limitations in 4096 policy space (ambiguity, inefficiency).
-   [x] Designed **PrometheusNet** with:
    -   Factorized `from Ã— to` policy output (64 Ã— 64).
    -   Separate **promotion head** (`None`, Q, R, B, N).
    -   Stable `value` head with `tanh` output scaling.
-   [x] Integrated `MCTS` stub into AegisNet for future planning inference.

---

### âš™ï¸ Data Generation & Training Strategy

-   [x] Built **StockfishDualTrainer**:
    -   White = Lv5, Black = Lv0 (configurable).
    -   Evaluation from Stockfish with `tanh`-scaled CP score for `value`.
    -   Top-k move sampling via `multipv` â†’ soft target label distribution.
    -   Label smoothing and strong weighting to top move.
-   [x] Enhanced training data to include:
    -   Soft policy target (`64x64`).
    -   Promotion label (one-hot of 5).
    -   Evaluation (`tanh(CP/400)`).

---

### ğŸ— Training Framework

-   [x] Built `Trainer` class with:
    -   Curriculum support (phased hybrid training).
    -   Logging for policy/value/promotion losses.
    -   Save/load models & weights cleanly.
-   [x] Implemented fallback compile flow for PrometheusNet vs AegisNet.
-   [x] Added dummy input call to avoid â€œmodel not builtâ€ error before saving weights.

---

### ğŸ§ª Training Observations

-   [x] Initial training losses:
    -   Policy â‰ˆ 0.12â€“0.13 âœ…
    -   Value â‰ˆ 0.59â€“0.66 âœ…
    -   Promotion â‰ˆ 7.0â€“7.4 â—ï¸(as expected due to rarity)
-   [x] Validated stable learning curve across 50 iterations with `num_games=20â€“40`.

---

### ğŸ§  Insights & Optimization

-   [x] Identified **promotion loss imbalance** and mitigated with:
    -   Loss weight adjustment (`promotion: 0.2`).
-   [x] Skipped neutral positions when `|value| < 0.05` (optional toggle).
-   [x] Boosted learning signal with:
    -   Weighted top-move policy injection.
    -   Temperature-scaled softmax (`temp=150`).

---

### ğŸ§© Next Steps

-   [ ] Integrate PrometheusNet with **MCTS** rollout via `predict()` method.
-   [ ] Add game-level ELO evaluation via test matches.
-   [ ] Convert PrometheusNet to **ONNX** for Raspberry Pi deployment.
-   [ ] Add live GUI + visualization to play against Prometheus.

---

**Current Verdict:**  
Athenaâ€™s policy is sharp. Prometheus is trained and stable. Deployment and evaluation next.
